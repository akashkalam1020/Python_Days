{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74543c16",
   "metadata": {},
   "source": [
    "<font color = darkblue size =6.5><center> Duplicate Question Pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f81702d",
   "metadata": {},
   "source": [
    "### Problem statement\n",
    "\n",
    "    It is a binary classification problem, for a given pair of questions we need to predict if they are duplicate or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58749274",
   "metadata": {},
   "source": [
    "### Import Laibraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ba1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a90aaf",
   "metadata": {},
   "source": [
    "#### Steps to Perform\n",
    "\n",
    "Step 1 : Data Analysis, Preprocessing, Model Training, Model Evaluation. (Step 1 - Simple Model.ipynb)\n",
    "\n",
    "Step 2 : Data Analysis, Preprocessing, Feature Engineering, Model Training, Model Evaluation.\n",
    "  \n",
    "Step 3 : Data Analysis, Preprocessing, Advanced Feature Engineering, Model Training, Model Evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9d48d",
   "metadata": {},
   "source": [
    "### Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32dc707",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('Questions.csv')\n",
    "print(\"shape\",new_df.shape)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values present in dataset\n",
    "new_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b93f994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed these null values\n",
    "new_df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cbed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "new_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05001dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of duplicate and non-duplicate questions\n",
    "\n",
    "print(new_df['is_duplicate'].value_counts())\n",
    "print((new_df['is_duplicate'].value_counts()/new_df['is_duplicate'].count()) * 100)\n",
    "new_df['is_duplicate'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfc472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique & Repeated questions\n",
    "\n",
    "qid = pd.Series(new_df['qid1'].tolist() + new_df['qid2'].tolist()) # Both the columns are combined together\n",
    "print('Number of unique questions over 808574 questions :',np.unique(qid).shape[0])\n",
    "x = qid.value_counts() > 1\n",
    "print('Number of repeated questions : ',x[x].shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486070d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeated questions histogram\n",
    "# There is a question which repeated 160 times and another repeated 120 times\n",
    "\n",
    "plt.hist(qid.value_counts().values,bins=160)\n",
    "plt.xlabel('Charecters in a question')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5123f691",
   "metadata": {},
   "source": [
    "### Text Wrangling and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb269e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(q):\n",
    "    q = str(q).lower().strip()\n",
    "    \n",
    "    # Replace certain special characters with their string equivalents\n",
    "    q = q.replace('%', ' percent')\n",
    "    q = q.replace('$', ' dollar ')\n",
    "    q = q.replace('₹', ' rupee ')\n",
    "    q = q.replace('€', ' euro ')\n",
    "    q = q.replace('@', ' at ')\n",
    "    \n",
    "    # The pattern '[math]' appears around 900 times in the whole dataset.\n",
    "    q = q.replace('[math]', '')\n",
    "    \n",
    "    # Replacing some numbers with string equivalents (not perfect, can be done better to account for more cases)\n",
    "    q = q.replace(',000,000,000 ', 'b ')\n",
    "    q = q.replace(',000,000 ', 'm ')\n",
    "    q = q.replace(',000 ', 'k ')\n",
    "    q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n",
    "    q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n",
    "    q = re.sub(r'([0-9]+)000', r'\\1k', q)\n",
    "    \n",
    "    from contractions import contraction\n",
    "    q_decontracted = []\n",
    "\n",
    "    for word in q.split():\n",
    "        if word in contraction:\n",
    "            word = contraction[word]\n",
    "        q_decontracted.append(word)\n",
    "            \n",
    "    q = ' '.join(q_decontracted)\n",
    "    q = q.replace(\"'ve\", \" have\")\n",
    "    q = q.replace(\"n't\", \" not\")\n",
    "    q = q.replace(\"'re\", \" are\")\n",
    "    q = q.replace(\"'ll\", \" will\")\n",
    "\n",
    "    \n",
    "    # Removing HTML tags\n",
    "    \n",
    "    q = BeautifulSoup(q)\n",
    "    q = q.get_text()\n",
    "    \n",
    "    # Remove punctuations\n",
    "    pattern = re.compile('\\W')\n",
    "    q = re.sub(pattern, ' ', q).strip()\n",
    "\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(\"I've already! wasn't <b>done</b>?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['question1'] = new_df['question1'].apply(preprocess)\n",
    "new_df['question2'] = new_df['question2'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c67110a",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "    \n",
    "    Creating some basics features which can help to improve accuracy : -\n",
    "        \n",
    "        1. Length of a question.\n",
    "        \n",
    "        2. Words in a question.\n",
    "        \n",
    "        3. Common words in a question.\n",
    "        \n",
    "        4. Total words in a question.\n",
    "        \n",
    "        5. word_share - This is the ratio of the number of common words to the length of the total words in a question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccdba02",
   "metadata": {},
   "source": [
    "#### 1. Feature of Length of a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51cce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['q1_len'] = new_df['question1'].str.len() \n",
    "new_df['q2_len'] = new_df['question2'].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3f5e7",
   "metadata": {},
   "source": [
    "#### 2. Feature of Words in a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f0d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['q1_num_words'] = new_df['question1'].apply(lambda row: len(row.split(\" \")))\n",
    "new_df['q2_num_words'] = new_df['question2'].apply(lambda row: len(row.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f36cdd5",
   "metadata": {},
   "source": [
    "#### 3. Feature of Common words in a question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a6d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_words(row):\n",
    "    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "    return len(w1 & w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd82cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['word_common'] = new_df.apply(common_words, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade4ec96",
   "metadata": {},
   "source": [
    "####  4. Feature of Total words in a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6884592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_words(row):\n",
    "    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "    return (len(w1) + len(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d9a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['word_total'] = new_df.apply(total_words, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4889f171",
   "metadata": {},
   "source": [
    "#### 5. Feature of word_share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e459217",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['word_share'] = round(new_df['word_common']/new_df['word_total'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eef2c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b50162",
   "metadata": {},
   "source": [
    "### Analysis of Basic Created Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c60763",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(new_df['q1_len'])\n",
    "print('minimum characters',new_df['q1_len'].min())\n",
    "print('maximum characters',new_df['q1_len'].max())\n",
    "print('average num of characters',int(new_df['q1_len'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(new_df['q2_len'])\n",
    "print('minimum characters',new_df['q2_len'].min())\n",
    "print('maximum characters',new_df['q2_len'].max())\n",
    "print('average num of characters',int(new_df['q2_len'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d2a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(new_df['q1_num_words'])\n",
    "print('minimum words',new_df['q1_num_words'].min())\n",
    "print('maximum words',new_df['q1_num_words'].max())\n",
    "print('average num of words',int(new_df['q1_num_words'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(new_df['q2_num_words'])\n",
    "print('minimum words',new_df['q2_num_words'].min())\n",
    "print('maximum words',new_df['q2_num_words'].max())\n",
    "print('average num of words',int(new_df['q2_num_words'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9ab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common words\n",
    "sns.distplot(new_df[new_df['is_duplicate'] == 0]['word_common'],label='non duplicate')\n",
    "sns.distplot(new_df[new_df['is_duplicate'] == 1]['word_common'],label='duplicate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# No conclusion all the data is overlaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a035a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total words\n",
    "sns.distplot(new_df[new_df['is_duplicate'] == 0]['word_total'],label='non duplicate')\n",
    "sns.distplot(new_df[new_df['is_duplicate'] == 1]['word_total'],label='duplicate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# if the words are greater than 25 then there will be low posibility of question to be duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489055cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word share\n",
    "sns.distplot(new_df[new_df['is_duplicate'] == 0]['word_share'],label='non duplicate')\n",
    "sns.distplot(new_df[new_df['is_duplicate'] == 1]['word_share'],label='duplicate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# If the value of word_share is <= 0.2 then there will be high probability of non-duplication of question and if greater\n",
    "# then high probability of duplication of question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb90dc3",
   "metadata": {},
   "source": [
    "### Advanced Feature Engineering\n",
    "\n",
    "### 1. Token Features\n",
    "\n",
    "cwc_min: This is the ratio of the number of common words to the length of the smaller question\n",
    "\n",
    "cwc_max: This is the ratio of the number of common words to the length of the larger question\n",
    "\n",
    "csc_min: This is the ratio of the number of common stop words to the smaller stop word count among the two questions\n",
    "\n",
    "csc_max: This is the ratio of the number of common stop words to the larger stop word count among the two questions\n",
    "\n",
    "ctc_min: This is the ratio of the number of common tokens to the smaller token count among the two questions\n",
    "\n",
    "ctc_max: This is the ratio of the number of common tokens to the larger token count among the two questions\n",
    "\n",
    "last_word_eq: 1 if the last word in the two questions is same, 0 otherwise\n",
    "\n",
    "first_word_eq: 1 if the first word in the two questions is same, 0 otherwise\n",
    "\n",
    "\n",
    "###  2. Fuzzy Features\n",
    "\n",
    "fuzz_ratio: fuzz_ratio score from fuzzywuzzy\n",
    "\n",
    "fuzz_partial_ratio: fuzz_partial_ratio from fuzzywuzzy\n",
    "\n",
    "token_sort_ratio: token_sort_ratio from fuzzywuzzy\n",
    "\n",
    "token_set_ratio: token_set_ratio from fuzzywuzzy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fca677",
   "metadata": {},
   "source": [
    "#### 1. Token Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f9d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_token_features(row):\n",
    "    \n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    SAFE_DIV = 0.0001 \n",
    "\n",
    "    STOP_WORDS = stopwords.words(\"english\")\n",
    "    \n",
    "    token_features = [0.0]*8\n",
    "    \n",
    "    # Converting the Sentence into Tokens: \n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "    \n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "\n",
    "    # Get the non-stopwords in Questions\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "    \n",
    "    #Get the stopwords in Questions\n",
    "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "    \n",
    "    # Get the common non-stopwords from Question pair\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "    \n",
    "    # Get the common stopwords from Question pair\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    \n",
    "    # Get the common Tokens from Question pair\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "    \n",
    "    \n",
    "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    \n",
    "    # Last word of both question is same or not\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "    \n",
    "    # First word of both question is same or not\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
    "    \n",
    "    return token_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4584aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_features = new_df.apply(fetch_token_features, axis=1)\n",
    "\n",
    "new_df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n",
    "new_df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n",
    "new_df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n",
    "new_df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n",
    "new_df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n",
    "new_df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n",
    "new_df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n",
    "new_df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bcda11",
   "metadata": {},
   "source": [
    "#### 2. Fuzzy Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f3fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_fuzzy_features(row):\n",
    "    \n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    fuzzy_features = [0.0]*4\n",
    "    \n",
    "    # fuzz_ratio\n",
    "    fuzzy_features[0] = fuzz.QRatio(q1, q2)\n",
    "\n",
    "    # fuzz_partial_ratio\n",
    "    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)\n",
    "\n",
    "    # token_sort_ratio\n",
    "    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)\n",
    "\n",
    "    # token_set_ratio\n",
    "    fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)\n",
    "\n",
    "    return fuzzy_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c908251",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_features = new_df.apply(fetch_fuzzy_features, axis=1)\n",
    "\n",
    "new_df['fuzz_ratio'] = list(map(lambda x: x[0], fuzzy_features))\n",
    "new_df['fuzz_partial_ratio'] = list(map(lambda x: x[1], fuzzy_features))\n",
    "new_df['token_sort_ratio'] = list(map(lambda x: x[2], fuzzy_features))\n",
    "new_df['token_set_ratio'] = list(map(lambda x: x[3], fuzzy_features))\n",
    "\n",
    "print(new_df.shape)\n",
    "new_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36924ecd",
   "metadata": {},
   "source": [
    "### Analysis of Advanced Created Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c205a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(new_df[['ctc_min', 'cwc_min', 'csc_min', 'is_duplicate']],hue='is_duplicate')\n",
    "\n",
    "# all the features 'ctc_min', 'cwc_min', 'csc_min' are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136389cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(new_df[['ctc_max', 'cwc_max', 'csc_max', 'is_duplicate']],hue='is_duplicate')\n",
    "# Here also all the features are important because they are not overloping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2b554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(new_df[['last_word_eq', 'first_word_eq', 'is_duplicate']],hue='is_duplicate')\n",
    "\n",
    "# If last word or first word is not matching then then there will be high probability of questin isn't duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234c0972",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(new_df[['fuzz_ratio', 'fuzz_partial_ratio','token_sort_ratio','token_set_ratio', 'is_duplicate']],hue='is_duplicate')\n",
    "# Here also all the features are important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created new_df for sample of 60000 questions.\n",
    "\n",
    "new_df = new_df.sample(30000, random_state=42)\n",
    "print(\"shape\",new_df.shape)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88363c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created ques_df which contain all the questions from question1 and question2\n",
    "\n",
    "ques_df = new_df[['question1','question2']]\n",
    "questions = list(ques_df['question1']) + list(ques_df['question2'])\n",
    "\n",
    "new_features = new_df.drop(columns=['id','qid1','qid2','question1','question2'])\n",
    "print(new_features.shape)\n",
    "new_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8242af",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer performing well as compare to TfidfVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=3000)\n",
    "q1_arr, q2_arr = np.vsplit(cv.fit_transform(questions).toarray(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df1 = pd.DataFrame(q1_arr, index= ques_df.index)\n",
    "temp_df2 = pd.DataFrame(q2_arr, index= ques_df.index)\n",
    "temp_df = pd.concat([temp_df1, temp_df2], axis=1)\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8032f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([new_features, temp_df], axis=1)\n",
    "print(final_df.shape)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b08b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.astype(np.uint8)\n",
    "\n",
    "# used astype(np.uint8) to avoid MemoryError: Unable to allocate 283. GiB for an array with shape (156816, 36, 53806)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac8a96",
   "metadata": {},
   "source": [
    "### Train_Test_Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41baa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(final_df.iloc[:,1:].values,final_df.iloc[:,0].values,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93384756",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "    \n",
    "    RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2984e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train,y_train)\n",
    "y_pred = rf.predict(x_test)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "\n",
    "print('confusion_matrix \\n',confusion_matrix(y_test,y_pred))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken for execution : {end_time - start_time} Seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23a3d39",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "    \n",
    "    MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5dc689",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(x_train,y_train)\n",
    "y_pred = mnb.predict(x_test)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "\n",
    "print('confusion_matrix \\n',confusion_matrix(y_test,y_pred))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken for execution : {end_time - start_time} Seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4983e98",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "    BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b87c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(x_train,y_train)\n",
    "y_pred = bnb.predict(x_test)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "\n",
    "print('confusion_matrix \\n',confusion_matrix(y_test,y_pred))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken for execution : {end_time - start_time} Seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2add1195",
   "metadata": {},
   "source": [
    "### After Performing All the Steps\n",
    "\n",
    "#### Step 1 : Analysis of data, preprocessing, Model Training, Evaluation\n",
    "\n",
    "        RandomForestClassifier = 0.7574\n",
    "        MultinomialNB = 0.7121\n",
    "        BernoulliNB = 0.7024\n",
    "    \n",
    "#### Step 2 : Analysis of data, preprocessing, Basic Feature Engineering, Model Training, Evaluation\n",
    "\n",
    "    RandomForestClassifier = 0.7595\n",
    "    MultinomialNB = 0.7413\n",
    "    BernoulliNB = 0.7045\n",
    "    \n",
    "#### Step 3 : Analysis of data, preprocessing, Advanced Feature Engineering, Model Training, Evaluation\n",
    "\n",
    "    RandomForestClassifier = 0.7918\n",
    "    MultinomialNB = 0.679\n",
    "    BernoulliNB = 0.733"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae812b",
   "metadata": {},
   "source": [
    "# Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e0d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(q):\n",
    "    q = str(q).lower().strip()\n",
    "    \n",
    "    # Replace certain special characters with their string equivalents\n",
    "    q = q.replace('%', ' percent')\n",
    "    q = q.replace('$', ' dollar ')\n",
    "    q = q.replace('₹', ' rupee ')\n",
    "    q = q.replace('€', ' euro ')\n",
    "    q = q.replace('@', ' at ')\n",
    "    \n",
    "    # The pattern '[math]' appears around 900 times in the whole dataset.\n",
    "    q = q.replace('[math]', '')\n",
    "    \n",
    "    # Replacing some numbers with string equivalents (not perfect, can be done better to account for more cases)\n",
    "    q = q.replace(',000,000,000 ', 'b ')\n",
    "    q = q.replace(',000,000 ', 'm ')\n",
    "    q = q.replace(',000 ', 'k ')\n",
    "    q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n",
    "    q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n",
    "    q = re.sub(r'([0-9]+)000', r'\\1k', q)\n",
    "    \n",
    "    from contractions import contraction\n",
    "    q_decontracted = []\n",
    "\n",
    "    for word in q.split():\n",
    "        if word in contraction:\n",
    "            word = contraction[word]\n",
    "        q_decontracted.append(word)\n",
    "            \n",
    "    q = ' '.join(q_decontracted)\n",
    "    q = q.replace(\"'ve\", \" have\")\n",
    "    q = q.replace(\"n't\", \" not\")\n",
    "    q = q.replace(\"'re\", \" are\")\n",
    "    q = q.replace(\"'ll\", \" will\")\n",
    "\n",
    "    \n",
    "    # Removing HTML tags\n",
    "    \n",
    "    q = BeautifulSoup(q)\n",
    "    q = q.get_text()\n",
    "    \n",
    "    # Remove punctuations\n",
    "    pattern = re.compile('\\W')\n",
    "    q = re.sub(pattern, ' ', q).strip()\n",
    "\n",
    "    \n",
    "    return q\n",
    "\n",
    "def test_common_words(q1,q2):\n",
    "    w1 = set(map(lambda word: word.lower().strip(), q1.split(\" \")))\n",
    "    w2 = set(map(lambda word: word.lower().strip(), q2.split(\" \")))    \n",
    "    return len(w1 & w2)\n",
    "\n",
    "def test_total_words(q1,q2):\n",
    "    w1 = set(map(lambda word: word.lower().strip(), q1.split(\" \")))\n",
    "    w2 = set(map(lambda word: word.lower().strip(), q2.split(\" \")))    \n",
    "    return (len(w1) + len(w2))\n",
    "\n",
    "def test_fetch_token_features(q1,q2):\n",
    "    \n",
    "    SAFE_DIV = 0.0001 \n",
    "\n",
    "    STOP_WORDS = stopwords.words(\"english\")\n",
    "    \n",
    "    token_features = [0.0]*8\n",
    "    \n",
    "    # Converting the Sentence into Tokens: \n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "    \n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "\n",
    "    # Get the non-stopwords in Questions\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "    \n",
    "    #Get the stopwords in Questions\n",
    "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "    \n",
    "    # Get the common non-stopwords from Question pair\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "    \n",
    "    # Get the common stopwords from Question pair\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    \n",
    "    # Get the common Tokens from Question pair\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "    \n",
    "    \n",
    "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    \n",
    "    # Last word of both question is same or not\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "    \n",
    "    # First word of both question is same or not\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
    "    \n",
    "    return token_features\n",
    "\n",
    "def test_fetch_fuzzy_features(q1,q2):\n",
    "    \n",
    "    fuzzy_features = [0.0]*4\n",
    "    \n",
    "    # fuzz_ratio\n",
    "    fuzzy_features[0] = fuzz.QRatio(q1, q2)\n",
    "\n",
    "    # fuzz_partial_ratio\n",
    "    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)\n",
    "\n",
    "    # token_sort_ratio\n",
    "    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)\n",
    "\n",
    "    # token_set_ratio\n",
    "    fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)\n",
    "\n",
    "    return fuzzy_features\n",
    "\n",
    "\n",
    "def query_point_creator(q1,q2):\n",
    "    \n",
    "    input_query = []\n",
    "    \n",
    "    # preprocess\n",
    "    q1 = preprocess(q1)\n",
    "    q2 = preprocess(q2)\n",
    "    \n",
    "    # fetch basic features\n",
    "    input_query.append(len(q1))\n",
    "    input_query.append(len(q2))\n",
    "    \n",
    "    input_query.append(len(q1.split(\" \")))\n",
    "    input_query.append(len(q2.split(\" \")))\n",
    "    \n",
    "    input_query.append(test_common_words(q1,q2))\n",
    "    input_query.append(test_total_words(q1,q2))\n",
    "    input_query.append(round(test_common_words(q1,q2)/test_total_words(q1,q2),2))\n",
    "    \n",
    "    # fetch token features\n",
    "    token_features = test_fetch_token_features(q1,q2)\n",
    "    input_query.extend(token_features)\n",
    "    \n",
    "    # fetch fuzzy features\n",
    "    fuzzy_features = test_fetch_fuzzy_features(q1,q2)\n",
    "    input_query.extend(fuzzy_features)\n",
    "    \n",
    "    # bow feature for q1\n",
    "    q1_bow = cv.transform([q1]).toarray()\n",
    "    \n",
    "    # bow feature for q2\n",
    "    q2_bow = cv.transform([q2]).toarray()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return np.hstack((np.array(input_query).reshape(1,19),q1_bow,q2_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6773b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not duplicate\n",
    "\n",
    "q22, q_22 = df['question1'][22], df['question2'][22] \n",
    "print(q22,'\\n', q_22)\n",
    "\n",
    "print()\n",
    "\n",
    "q26, q_26 = df['question1'][26], df['question2'][26] \n",
    "print(q26,'\\n', q_26)\n",
    "\n",
    "print('prediction : ',rf.predict(query_point_creator(q22,q_22))[0])\n",
    "print('prediction : ',rf.predict(query_point_creator(q26,q_26))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87d285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate\n",
    "\n",
    "eleven, eleven1 = df['question1'][11], df['question2'][11]\n",
    "print(eleven,'\\n', eleven1)\n",
    "\n",
    "print()\n",
    "\n",
    "seven, seven1 = df['question1'][7], df['question2'][7]\n",
    "print(seven,'\\n', seven1)\n",
    "\n",
    "print('prediction : ',rf.predict(query_point_creator(eleven,eleven1))[0])\n",
    "print('prediction : ',rf.predict(query_point_creator(seven,seven1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4881fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(rf,open('model.pkl','wb'))\n",
    "pickle.dump(cv,open('cv.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
